{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "The goal of this analysis is to map existing datasets, identify possible correlations between them, and determine if, for example, one dataset is a subset of another. The result of the data analysis should be a new dataset that can be used for import into the database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "In this section, we define the dependencies on the other libraries that will be needed for the analysis. This includes the installation of the libraries used, which are defined by default in the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "requirementsPath = os.path.join(os.path.dirname(os.path.realpath('__file__')),\"requirements.txt\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirementsPath])\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib_venn import venn4\n",
    "from venny4py.venny4py import *\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, Markdown, IFrame\n",
    "import seaborn as sns\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archives(Enum):\n",
    "    MZA = 'mza'\n",
    "    HP_PRAHA = 'hpPraha'\n",
    "    SOA_HRADEC_KRALOVE = 'soaHradecKralove'\n",
    "    SOA_LITOMERICE = 'soaLitomerice'\n",
    "    ZA_OPAVA = 'zaOpava'\n",
    "    SOA_PLZEN = 'soaPlzen'\n",
    "    SOA_PRAHA = 'soaPraha'\n",
    "    SOA_TREBON = 'soaTrebon'\n",
    "\n",
    "class Dataframes(Enum):\n",
    "    POP = 'pop'\n",
    "    SOKOLIK = 'sokolik'\n",
    "    VALUSEK = 'valusek'\n",
    "    SCANS = 'scans'\n",
    "\n",
    "class Collections(Enum):\n",
    "    DATASET = \"dataset\"\n",
    "    SIZE = \"size\"\n",
    "    SIG_NULL = \"sig_null\"\n",
    "    INV_NULL = \"inv_null\"\n",
    "    RUIAN_NULL = \"ruian_null\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "In this section, we import JSON data files into python structures that we will then work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_uzemi(matrika):\n",
    "    uzemi = matrika.get('uzemi')\n",
    "    if uzemi:\n",
    "        transformed_uzemi = [{\"name\": name, **data} for name, data in uzemi.items()]\n",
    "        return {**matrika, 'uzemi': transformed_uzemi}\n",
    "    return matrika  # Return the original matrika if 'uzemi' is missing or None\n",
    "\n",
    "def loadDataFrameFromJson(filePath, key = None):\n",
    "    dataJson = pd.read_json(filePath)\n",
    "    if(key != None):\n",
    "        dataJson = dataJson[key]\n",
    "    else:\n",
    "        dataJson = pd.Series(dataJson.to_dict(orient='records'))\n",
    "\n",
    "    dataJson = dataJson.apply(transform_uzemi)\n",
    "\n",
    "    return pd.json_normalize(dataJson)\n",
    "\n",
    "dataframes = {\n",
    "    Dataframes.POP: {\n",
    "        Archives.MZA: {},\n",
    "        Archives.HP_PRAHA: {},\n",
    "        Archives.SOA_HRADEC_KRALOVE: {},\n",
    "        Archives.SOA_LITOMERICE: {},\n",
    "        Archives.ZA_OPAVA: {},\n",
    "        Archives.SOA_PLZEN: {},\n",
    "        Archives.SOA_PRAHA: {},\n",
    "        Archives.SOA_TREBON: {},\n",
    "    },\n",
    "    Dataframes.SOKOLIK: {        \n",
    "        Archives.MZA: {},\n",
    "        Archives.HP_PRAHA: {},\n",
    "        Archives.SOA_HRADEC_KRALOVE: {},\n",
    "        Archives.SOA_LITOMERICE: {},\n",
    "        Archives.ZA_OPAVA: {},\n",
    "        Archives.SOA_PLZEN: {},\n",
    "        Archives.SOA_PRAHA: {},\n",
    "        Archives.SOA_TREBON: {},\n",
    "    },\n",
    "    Dataframes.VALUSEK: {\n",
    "        Archives.MZA: {},\n",
    "        Archives.HP_PRAHA: {},\n",
    "        Archives.SOA_HRADEC_KRALOVE: {},\n",
    "        Archives.SOA_LITOMERICE: {},\n",
    "        Archives.ZA_OPAVA: {},\n",
    "        Archives.SOA_PLZEN: {},\n",
    "        Archives.SOA_PRAHA: {},\n",
    "        Archives.SOA_TREBON: {},\n",
    "    },\n",
    "    Dataframes.SCANS: {\n",
    "        Archives.MZA: {},\n",
    "        Archives.HP_PRAHA: {},\n",
    "        Archives.SOA_HRADEC_KRALOVE: {},\n",
    "        Archives.SOA_LITOMERICE: {},\n",
    "        Archives.ZA_OPAVA: {},\n",
    "        Archives.SOA_PLZEN: {},\n",
    "        Archives.SOA_PRAHA: {},\n",
    "        Archives.SOA_TREBON: {},\n",
    "    }\n",
    "}\n",
    "\n",
    "dataframes[Dataframes.POP][Archives.MZA][Collections.DATASET] =  loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Brno_formated.json', \"matriky\") \n",
    "dataframes[Dataframes.POP][Archives.HP_PRAHA][Collections.DATASET] =  loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/HLPraha_formated.json', \"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.SOA_HRADEC_KRALOVE][Collections.DATASET] =  loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Hradec_formated.json', \"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.SOA_LITOMERICE][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Litomerice_formated.json',\"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.ZA_OPAVA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Opava_formated.json',\"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.SOA_PLZEN][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Plzen_formated.json', \"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.SOA_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Praha_formated.json', \"matriky\")\n",
    "dataframes[Dataframes.POP][Archives.SOA_TREBON][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Dominik_Pop/Trebon_formated.json', \"matriky\")\n",
    "print(\"Pop dataframes imported successfully\")\n",
    "\n",
    "dataframes[Dataframes.SOKOLIK][Archives.MZA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/mza.json', \"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.HP_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/hlPraha.json', \"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.SOA_HRADEC_KRALOVE][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/soaHradecKralove.json',\"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.SOA_LITOMERICE][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/soaLitomerice.json',\"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.ZA_OPAVA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/zaOpava.json',\"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.SOA_PLZEN][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/soaPlzen.json', \"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.SOA_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/soaPraha.json',\"records\")\n",
    "dataframes[Dataframes.SOKOLIK][Archives.SOA_TREBON][Collections.DATASET] = loadDataFrameFromJson('../datasets/parovani_ruian/Sokolik/soaTrebon.json',\"records\")\n",
    "print(\"Sokolik dataframes imported successfully\")\n",
    "\n",
    "dataframes[Dataframes.VALUSEK][Archives.MZA][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/mza_registers_16.03.2023-12_35_20.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.HP_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/pragapublica_registers_15.03.2023-13_25_11.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.SOA_HRADEC_KRALOVE][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/aron_registers_16.03.2023-16_15_06.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.SOA_LITOMERICE][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/vademecum_registers_29.03.2023-17_33_26.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.ZA_OPAVA][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/archives_registers_29.03.2023-13_01_28.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.SOA_PLZEN][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/portafontium_registers_15.03.2023-11_33_48.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.SOA_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/ebadatelna_registers_15.03.2023-12_13_58.json')\n",
    "dataframes[Dataframes.VALUSEK][Archives.SOA_TREBON][Collections.DATASET] = loadDataFrameFromJson('../datasets/Scraped/ceskearchivy_registers_15.03.2023-11_09_40.json')\n",
    "print(\"Valousek dataframes imported successfully\")\n",
    "\n",
    "dataframes[Dataframes.SCANS][Archives.MZA][Collections.DATASET] =  loadDataFrameFromJson('../datasets/matriky/actapublica/actapublica.json', \"matriky\") \n",
    "dataframes[Dataframes.SCANS][Archives.HP_PRAHA][Collections.DATASET] =  loadDataFrameFromJson('../datasets/matriky/ahmp/ahmp.json', \"matriky\")\n",
    "dataframes[Dataframes.SCANS][Archives.SOA_HRADEC_KRALOVE][Collections.DATASET] = pd.DataFrame()\n",
    "dataframes[Dataframes.SCANS][Archives.SOA_LITOMERICE][Collections.DATASET] = loadDataFrameFromJson('../datasets/matriky/soaLitomerice/litomerice.json', \"matriky\")\n",
    "dataframes[Dataframes.SCANS][Archives.ZA_OPAVA][Collections.DATASET] = loadDataFrameFromJson('../datasets/matriky/zaoOpava/opava.json',\"matriky\")\n",
    "dataframes[Dataframes.SCANS][Archives.SOA_PLZEN][Collections.DATASET] = loadDataFrameFromJson('../datasets/matriky/plzen/plzen.json', \"matriky\")\n",
    "dataframes[Dataframes.SCANS][Archives.SOA_PRAHA][Collections.DATASET] = loadDataFrameFromJson('../datasets/matriky/soaPraha/soaPraha.json',\"matriky\")\n",
    "dataframes[Dataframes.SCANS][Archives.SOA_TREBON][Collections.DATASET] = loadDataFrameFromJson('../datasets/matriky/trebon/trebon.json', \"matriky\")\n",
    "print(\"Scan dataframes imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sizes of datasets\n",
    "In this section, we'll look at dataset sizes and how much they differ from each other. We can see that the dataset provided by Dominik Pop is significantly shorter even though it contains more detailed information about each archival item.  So now we will check whether Dominik Pop's dataset is covered in the second dataset. According to the structure of the data, it is obvious that Dominik Pop collected data only on civil registers, while Jakub Sokolík also collected other types of archival material such as tax rolls and censuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the sizes of the collections\n",
    "\n",
    "for dataSource in Dataframes:\n",
    "    for archive in Archives:\n",
    "        if dataSource == Dataframes.SOKOLIK:\n",
    "            df = dataframes[dataSource][archive][Collections.DATASET]\n",
    "            dataframes[dataSource][archive][Collections.SIZE] = len(df[df[\"type\"] == \"MAT\"])\n",
    "        else:\n",
    "            dataframes[dataSource][archive][Collections.SIZE] = len(dataframes[dataSource][archive][Collections.DATASET])\n",
    "\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.23\n",
    "\n",
    "# Create an array of x values for the bars\n",
    "x = np.arange(len(list(Archives)))\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "bar1 = plt.bar(x - 1.5 * bar_width, [dataframes[Dataframes.POP][archive][Collections.SIZE] for archive in Archives], bar_width, label=str(Dataframes.POP).split(\".\")[1])\n",
    "bar2 = plt.bar(x - 0.5 * bar_width, [dataframes[Dataframes.SOKOLIK][archive][Collections.SIZE] for archive in Archives], bar_width, label=str(Dataframes.SOKOLIK).split(\".\")[1])\n",
    "bar3 = plt.bar(x + 0.5 * bar_width, [dataframes[Dataframes.VALUSEK][archive][Collections.SIZE] for archive in Archives], bar_width, label=str(Dataframes.VALUSEK).split(\".\")[1])\n",
    "bar4 = plt.bar(x + 1.5 * bar_width, [dataframes[Dataframes.SCANS][archive][Collections.SIZE] for archive in Archives], bar_width, label=str(Dataframes.SCANS).split(\".\")[1])\n",
    "\n",
    "# Add labels with exact numbers above the bars\n",
    "for bar in [bar1, bar2, bar3, bar4]:\n",
    "    for value in bar:\n",
    "        height = value.get_height()\n",
    "        plt.annotate('{}'.format(height),  # Display the value\n",
    "                     xy=(value.get_x() + value.get_width() / 2, height),  # Position of the value above the bar\n",
    "                     xytext=(0, 3),  # Offset for the text position\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "# Label the x-axis and y-axis\n",
    "plt.xlabel(\"Collection Name\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "\n",
    "# Set the x-axis labels and rotate them by 90 degrees\n",
    "plt.xticks(x, [str(archive).split(\".\")[1] for archive in list(Archives)], rotation=90)\n",
    "\n",
    "# Set the title and legend\n",
    "plt.title(\"Collection Sizes Comparison\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('result/collectionSizesComparison.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed comparison of datasets\n",
    "In this section, we compare datasets at the archive level, comparing the rate of occurrence of empty values and whether the smaller dataset is included within the larger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObjectComparisonPrint(dataFrameName):\n",
    "    \"\"\"\n",
    "    Compare and print example records from two data frames side by side in an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    dataFrameName (str): The name of the data frame to be compared (e.g., 'mzaDf').\n",
    "\n",
    "    Example Usage:\n",
    "    ObjectComparisonPrint('mzaDf')\n",
    "    \n",
    "    Args:\n",
    "        dataFrameName (str): The name of the data frame to be compared.\n",
    "\n",
    "    Returns:\n",
    "        None: The function displays the HTML table, and there is no return value.\n",
    "    \"\"\"\n",
    "    dominikPopRecordExample = {}\n",
    "    if not dataframes[Dataframes.POP][dataFrameName].empty:\n",
    "        dominikPopRecord = dataframes[Dataframes.POP][dataFrameName].head(1).iloc[0].to_dict()\n",
    "        if \"uzemi\" in dominikPopRecord and len(dominikPopRecord['uzemi']) > 1:\n",
    "            dominikPopRecord['uzemi'] = [dominikPopRecord['uzemi'][0]]\n",
    "        dominikPopRecordExample = json.dumps(dominikPopRecord, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "    jakubSokolikRecordExample = {}\n",
    "    if not dataframes[Dataframes.SOKOLIK][dataFrameName].empty:\n",
    "        jakubSokolikRecordExample = json.dumps(dataframes[Dataframes.SOKOLIK][dataFrameName].head(1).iloc[0].to_dict(), ensure_ascii=False, indent=2)\n",
    "    \n",
    "\n",
    "    scansRecordExample = {}\n",
    "    if not dataframes[Dataframes.SCANS][dataFrameName].empty:\n",
    "        scansRecord = dataframes[Dataframes.SCANS][dataFrameName].head(1).iloc[0].to_dict()\n",
    "        if \"snimky.snimek\" in scansRecord and len(scansRecord['snimky.snimek']) > 1:\n",
    "            scansRecord['snimky.snimek'] = [scansRecord['snimky.snimek'][0]]\n",
    "\n",
    "        if \"snimky.url\" in scansRecord and len(scansRecord['snimky.url']) > 1:\n",
    "            scansRecord['snimky.url'] = [scansRecord['snimky.url'][0]]\n",
    "\n",
    "        scansRecordExample = json.dumps(scansRecord, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Create an HTML table to display the JSON objects side by side\n",
    "    #html_table = f'<h1>Record example for {str(dataFrameName).split(\".\")[1]}</h1><table><tr><th>Dominik Pop</th><th>Jakub Sokolik</th><th>Scans</th></tr><tr><td><pre>{dominikPopRecordExample}</pre></td><td><pre>{jakubSokolikRecordExample}</pre></td><td><pre>{scansRecordExample}</pre></td></tr></table>'\n",
    "\n",
    "    # Display the HTML table\n",
    "    #display(HTML(html_table))\n",
    "    \n",
    "    display(Markdown(\"### Comparison of data objects\"))\n",
    "    display(Markdown(\"#### Dominik Pop\"))\n",
    "    print(dominikPopRecordExample)\n",
    "    display(Markdown(\"#### Jakub Sokolík\"))\n",
    "    print(jakubSokolikRecordExample)\n",
    "    display(Markdown(\"#### Scans\"))\n",
    "    print(scansRecordExample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeMissingValues(dataFrameName):\n",
    "\n",
    "    def has_ruian(uzemi):\n",
    "        if isinstance(uzemi, list):\n",
    "            for item in uzemi:\n",
    "                if isinstance(item, dict) and \"ruian\" in item:\n",
    "                    return True\n",
    "        elif isinstance(uzemi, dict) and (\"obec_ruian\" in uzemi or \"cast_obce_ruian\" in uzemi):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    df = dataframes[Dataframes.POP][dataFrameName][Collections.DATASET]\n",
    "    dataframes[Dataframes.POP][dataFrameName][Collections.SIG_NULL] = len(df[df[\"signatura\"].isna() | (df[\"signatura\"] == '') | df[\"signatura\"].str.isspace()])\n",
    "    dataframes[Dataframes.POP][dataFrameName][Collections.INV_NULL] = len(df[df[\"invCislo\"].isna() | (df[\"invCislo\"] == '') | df[\"invCislo\"].str.isspace()])\n",
    "    dataframes[Dataframes.POP][dataFrameName][Collections.RUIAN_NULL] = len(df[df[\"uzemi\"].apply(has_ruian) == False])\n",
    "\n",
    "    df = dataframes[Dataframes.SOKOLIK][dataFrameName][Collections.DATASET]\n",
    "    df = df[df[\"type\"] == \"MAT\"]\n",
    "\n",
    "    dataframes[Dataframes.SOKOLIK][dataFrameName][Collections.SIG_NULL] = len(df[df[\"signature\"].isna() | (df[\"signature\"] == '') | df[\"signature\"].str.isspace()])\n",
    "    dataframes[Dataframes.SOKOLIK][dataFrameName][Collections.INV_NULL] = len(df[df[\"inv_id\"].isna() | (df[\"inv_id\"] == '') | df[\"inv_id\"].str.isspace()])\n",
    "    dataframes[Dataframes.SOKOLIK][dataFrameName][Collections.RUIAN_NULL] = len(df[df[\"obec_ruian\"].isna() | (df[\"obec_ruian\"] == '') | df[\"obec_ruian\"].str.isspace() | df[\"cast_obce_ruian\"].isna() | (df[\"cast_obce_ruian\"] == '') | df[\"cast_obce_ruian\"].str.isspace()])\n",
    "\n",
    "    df = dataframes[Dataframes.SCANS][dataFrameName][Collections.DATASET]\n",
    "    #lenScan = dataframes[Dataframes.SCANS][dataFrameName][Collections.SIZE]\n",
    "    \n",
    "    dataframes[Dataframes.SCANS][dataFrameName][Collections.SIG_NULL] = dataframes[Dataframes.SCANS][dataFrameName][Collections.SIZE]\n",
    "    if 'signatura' in df.columns:\n",
    "        dataframes[Dataframes.SCANS][dataFrameName][Collections.SIG_NULL] = len(df[df[\"signatura\"].isnull() | df[\"signatura\"].isna() | (df[\"signatura\"] == '') | df[\"signatura\"].str.isspace()])\n",
    "    \n",
    "    dataframes[Dataframes.SCANS][dataFrameName][Collections.INV_NULL] = dataframes[Dataframes.SCANS][dataFrameName][Collections.SIZE]\n",
    "    if 'inv. cislo' in df.columns:\n",
    "        dataframes[Dataframes.SCANS][dataFrameName][Collections.INV_NULL] = len(df[df[\"inv. cislo\"].isnull() | df[\"inv. cislo\"].isna() | (df[\"inv. cislo\"] == '') | df[\"inv. cislo\"].str.isspace()])\n",
    "    \n",
    "    dataframes[Dataframes.SCANS][dataFrameName][Collections.RUIAN_NULL] = dataframes[Dataframes.SCANS][dataFrameName][Collections.SIZE]\n",
    "    #if 'ruian' in df.columns:\n",
    "    #dataframes[Dataframes.SCANS][dataFrameName][Collections.RUIAN_NULL] = len(df[df[\"ruian\"].isnull()])\n",
    "\n",
    "    \n",
    "    df = dataframes[Dataframes.VALUSEK][dataFrameName][Collections.DATASET]\n",
    "    dataframes[Dataframes.VALUSEK][dataFrameName][Collections.SIG_NULL] = len(df[df[\"signature\"].isna() | (df[\"signature\"] == '') | (isinstance(df[\"signature\"], str) and df[\"signature\"].str.isspace())])\n",
    "    dataframes[Dataframes.VALUSEK][dataFrameName][Collections.INV_NULL] = len(df[df[\"inventory_number\"].isna() | (df[\"inventory_number\"] == '') | (isinstance(df[\"signature\"], str) and df[\"inventory_number\"].str.isspace())])\n",
    "    dataframes[Dataframes.VALUSEK][dataFrameName][Collections.RUIAN_NULL] = dataframes[Dataframes.VALUSEK][dataFrameName][Collections.SIZE]\n",
    "\n",
    "\n",
    "    data = {}\n",
    "    for author in Dataframes:\n",
    "        data[author] = [\n",
    "            dataframes[author][dataFrameName][Collections.SIZE],\n",
    "            dataframes[author][dataFrameName][Collections.SIG_NULL],\n",
    "            dataframes[author][dataFrameName][Collections.INV_NULL],\n",
    "            dataframes[author][dataFrameName][Collections.RUIAN_NULL],\n",
    "        ]\n",
    "    \n",
    "\n",
    "    x = np.arange(4)  # Adjust the number of bars as needed\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bar_width = 0.2  # Width of each bar\n",
    "    \n",
    "    bar1 = plt.bar(x  - 1.5 * bar_width, data[Dataframes.POP], bar_width, label=str(Dataframes.POP).split(\".\")[1])\n",
    "    bar2 = plt.bar(x - 0.5 * bar_width, data[Dataframes.SOKOLIK], bar_width, label=str(Dataframes.SOKOLIK).split(\".\")[1])\n",
    "    bar3 = plt.bar(x + 0.5 * bar_width, data[Dataframes.VALUSEK], bar_width, label=str(Dataframes.VALUSEK).split(\".\")[1])\n",
    "    bar4 = plt.bar(x + 1.5 * bar_width, data[Dataframes.SCANS], bar_width, label=str(Dataframes.SCANS).split(\".\")[1])\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel(\"Data Categories\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Record Counts and Null Value Counts for datasets for {str(dataFrameName).split('.')[1]}\")\n",
    "\n",
    "    # Set the x-axis labels and rotate them by 45 degrees for better readability\n",
    "    labels = ['Total Records', 'Signature Null', 'Inv Number Null', \"Ruian Null\"]\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "\n",
    "    # Add exact numbers above the bars\n",
    "    for bar, values in zip([bar1, bar2, bar3, bar4], [data[Dataframes.POP], data[Dataframes.SOKOLIK], data[Dataframes.VALUSEK], data[Dataframes.SCANS]]):\n",
    "        for i in range(len(x)):\n",
    "            plt.text(bar[i].get_x() + bar[i].get_width() / 2, values[i] + 5, str(values[i]), ha='center', va='bottom')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the bar graph\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'result/{dataFrameName.value}/missingValues.pdf', format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeDuplicates(dataFrameName):\n",
    "   \n",
    "    df_pop = dataframes[Dataframes.POP][dataFrameName][Collections.DATASET]\n",
    "    df_sok = dataframes[Dataframes.SOKOLIK][dataFrameName][Collections.DATASET]\n",
    "    df_val = dataframes[Dataframes.VALUSEK][dataFrameName][Collections.DATASET]\n",
    "    df_scan = dataframes[Dataframes.SCANS][dataFrameName][Collections.DATASET]\n",
    "    \n",
    "\n",
    "    # Find duplicates in \"signatura\" column\n",
    "    pop_signatura_duplicates = df_pop[df_pop.duplicated(subset=['signatura'], keep=False)]\n",
    "    sok_signature_duplicates = df_sok[df_sok[\"type\"] == \"MAT\"].duplicated(subset=['signature'], keep=False)\n",
    "    val_signatura_duplicates = df_val[df_val.duplicated(subset=['signature'], keep=False)]\n",
    "    scan_signature_duplicates = df_scan[df_scan.duplicated(subset=['signatura'], keep=False)]\n",
    "\n",
    "    # Find duplicates in \"invCislo\" column\n",
    "    pop_invcislo_duplicates = df_pop[df_pop.duplicated(subset=['invCislo'], keep=False)]\n",
    "    sok_inv_id_duplicates = df_sok[df_sok[\"type\"] == \"MAT\"].duplicated(subset=['inv_id'], keep=False)\n",
    "    val_inv_id_duplicates = df_val[df_val.duplicated(subset=['inventory_number'], keep=False)]\n",
    "\n",
    "\n",
    "    pop_signatura_duplicates_count = len(pop_signatura_duplicates)\n",
    "    sok_signature_duplicates_count = len(sok_signature_duplicates)\n",
    "    val_signature_duplicates_count = len(val_signatura_duplicates)\n",
    "    scan_signature_duplicates_count = len(scan_signature_duplicates)\n",
    "\n",
    "    pop_invcislo_duplicates_count = len(pop_invcislo_duplicates)\n",
    "    sok_inv_id_duplicates_count = len(sok_inv_id_duplicates)\n",
    "    val_inv_id_duplicates_count = len(val_inv_id_duplicates)\n",
    "    scan_inv_id_duplicates_count = 0\n",
    "    \n",
    "\n",
    "    x = ['Dominik Pop Signatura', 'Dominik Pop InvCislo', 'Jakub Sokolik Signature', 'Jakub Sokolik InvId','Jan Valusek Signatura', 'Jan Valusek InvCislo', \"Scans Signature\", \"Scans invId\"]\n",
    "    duplicates_data = [pop_signatura_duplicates_count, \n",
    "                       pop_invcislo_duplicates_count, \n",
    "                       sok_signature_duplicates_count, \n",
    "                       sok_inv_id_duplicates_count,\n",
    "                       val_signature_duplicates_count,\n",
    "                       val_inv_id_duplicates_count,\n",
    "                       scan_signature_duplicates_count, \n",
    "                       scan_inv_id_duplicates_count]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(x, duplicates_data, color=['blue', 'blue', 'orange', 'orange', 'green', 'green', 'red', 'red'])\n",
    "    plt.xlabel(\"Dataset and Column\")\n",
    "    plt.ylabel(\"Count of Duplicates\")\n",
    "    plt.title(f\"Count of Duplicates in 'signatura' and 'invCislo' Columns for {str(dataFrameName).split('.')[1]}\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Adding annotations to each bar\n",
    "    for bar, count in zip(bars, duplicates_data):\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'result/{dataFrameName.value}/duplicities.pdf', format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \"\"\"\n",
    "    scans_dict = {row.signatura: row for row in df_sca.itertuples()}\n",
    "    \n",
    "    \n",
    "    no_match_found_for_dominik_inv_cislo = []\n",
    "    # Initialize counters\n",
    "    match_found_between_pop_and_sokolik = 0\n",
    "    no_match_found_between_pop_and_sokolik = 0\n",
    "\n",
    "    \n",
    "    match_found_between_pop_and_sokolik_village = 0\n",
    "    no_match_found_between_pop_and_sokolik_village = 0\n",
    "\n",
    "    match_found_between_pop_and_sokolik_ruian = 0\n",
    "    no_match_found_between_pop_and_sokolik_ruian = 0\n",
    "\n",
    "    \n",
    "    ruian_none_dominik_pop = 0\n",
    "    ruian_set_dominik_pop = 0\n",
    "\n",
    "    \n",
    "    ruian_none_jakub_sokolik = 0\n",
    "    ruian_set_jakub_sokolik = 0\n",
    "\n",
    "    no_match_found_scans_for_pop = 0   \n",
    "    match_found_scans_for_pop = 0  \n",
    "\n",
    "    different_ruians = {}\n",
    "    no_match_found_scans_for_sokolik = 0\n",
    "    match_found_scans_for_sokolik = 0\n",
    "\n",
    "\n",
    "    dfSokKey = \"signature\"\n",
    "    sok_df = dataframes[Dataframes.SOKOLIK][dataFrameName]\n",
    "    sok_inv_null = len(sok_df[(sok_df[\"type\"] == \"MAT\") & (sok_df[\"inv_id\"].isna() | (sok_df[\"inv_id\"] == '') | sok_df[\"inv_id\"].str.isspace())])\n",
    "    if len(dataframes[Dataframes.SOKOLIK][dataFrameName])*0.05 > sok_inv_null:\n",
    "        dfSokKey = \"inv_id\"\n",
    "    \n",
    "    # Iterate through the records in Dominik Pop dataset\n",
    "    for dominik_row in df_pop.itertuples():\n",
    "        \n",
    "       \n",
    "        dominik_invCislo = dominik_row.invCislo\n",
    "        if dominik_invCislo is None or dominik_invCislo == \"\":\n",
    "            dominik_invCislo = dominik_row.signatura\n",
    "\n",
    "        # Find a matching record in Scans dataset\n",
    "        matching_scans_row = scans_dict.get(dominik_invCislo)\n",
    "\n",
    "        if matching_scans_row is None:\n",
    "            no_match_found_scans_for_pop += 1\n",
    "        else:\n",
    "            match_found_scans_for_pop += 1\n",
    "        \n",
    "        # Find a matching record in Sokolik dataset\n",
    "        matching_sokolik_rows = df_sok[df_sok[dfSokKey] == dominik_invCislo]\n",
    "\n",
    "        if matching_sokolik_rows.empty:\n",
    "            #print(\"no_match_found_between_pop_and_sokolik: \", dominik_invCislo)\n",
    "            no_match_found_for_dominik_inv_cislo.append(dominik_invCislo)\n",
    "            no_match_found_between_pop_and_sokolik += 1\n",
    "\n",
    "        else:\n",
    "            # Check ruian\n",
    "\n",
    "            match_found_between_pop_and_sokolik += 1\n",
    "\n",
    "            for uzemi in dominik_row.uzemi:\n",
    "                #print(uzemi)              \n",
    "\n",
    "                uzemi_match_rows = matching_sokolik_rows[(matching_sokolik_rows[\"obec\"] == uzemi[\"name\"]) | (matching_sokolik_rows[\"cast_obce\"] == uzemi[\"name\"])]\n",
    "                #print(\"done: \", type(uzemi_match_rows))\n",
    "                if uzemi_match_rows.empty:\n",
    "                    #print(\"Dominik pop uzemi not found in Sokolik dataset\")\n",
    "                    no_match_found_between_pop_and_sokolik_village += 1\n",
    "                else:\n",
    "                    match_found_between_pop_and_sokolik_village += 1\n",
    "                   \n",
    "                    uzemi_match_row = uzemi_match_rows.iloc[0]\n",
    "                    \n",
    "                    #print(\"uzemi_match_row: \", uzemi_match_row)\n",
    "                    if ((\"obec_ruian\" in uzemi_match_row) and (str(uzemi[\"ruian\"]) == uzemi_match_row[\"obec_ruian\"])) or ((\"cast_obce_ruian\" in uzemi_match_row) and (str(uzemi[\"ruian\"]) == uzemi_match_row[\"cast_obce_ruian\"])):\n",
    "                        match_found_between_pop_and_sokolik_ruian += 1\n",
    "                    else:\n",
    "                        \n",
    "                        no_match_found_between_pop_and_sokolik_ruian += 1\n",
    "                        \n",
    "                        ruianSets = True\n",
    "\n",
    "                        if uzemi[\"ruian\"] is None:\n",
    "                            ruian_none_dominik_pop += 1\n",
    "                            ruianSets = False\n",
    "                        else:\n",
    "                            ruian_set_dominik_pop += 1\n",
    "\n",
    "                        if \"obec_ruian\" in uzemi_match_row or \"cast_obce_ruian\" in uzemi_match_row:\n",
    "                            ruian_set_jakub_sokolik += 1\n",
    "                        else:\n",
    "                            ruian_none_jakub_sokolik += 1\n",
    "                            ruianSets = False\n",
    "\n",
    "                        if ruianSets:\n",
    "                            diff_key = str(uzemi[\"ruian\"])\n",
    "                            if (\"obec_ruian\" in uzemi_match_row) and (uzemi_match_row[\"obec_ruian\"] is not None):\n",
    "                                diff_key += \"_\"+uzemi_match_row[\"obec_ruian\"]\n",
    "\n",
    "                            if (\"cast_obce_ruian\" in uzemi_match_row) and (uzemi_match_row[\"cast_obce_ruian\"] is not None):\n",
    "                                diff_key += \"_\"+uzemi_match_row[\"cast_obce_ruian\"]\n",
    "                            \n",
    "                            different_ruians[diff_key] = {\n",
    "                                \"dominik_row\": dominik_row._asdict(),\n",
    "                                \"sokolik_row\": uzemi_match_row.to_dict()\n",
    "                            }\n",
    "                        \n",
    "\n",
    "    print(\"Unique different ruians: \", len(different_ruians))\n",
    "\n",
    "    \n",
    "    for sokolik_row in df_sok[df_sok[\"type\"] == \"MAT\"].itertuples():\n",
    "        sokolik_signature = sokolik_row.signature\n",
    "        # Find a matching record in Scans dataset\n",
    "        matching_scans_row = scans_dict.get(sokolik_signature)\n",
    "\n",
    "        if matching_scans_row is None:\n",
    "            no_match_found_scans_for_sokolik += 1\n",
    "        else:\n",
    "            match_found_scans_for_sokolik += 1\n",
    "\n",
    "\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    with open(f'result/{dataFrameName.value}/different_ruians.json', 'w') as json_file:\n",
    "        json.dump(different_ruians, json_file, default=convert_to_serializable, allow_nan=False, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    data = [\n",
    "        {'Only in Pop dataset': no_match_found_between_pop_and_sokolik, 'In both datasets': match_found_between_pop_and_sokolik},\n",
    "        {'Only in Pop dataset': no_match_found_between_pop_and_sokolik_village, 'In both datasets': match_found_between_pop_and_sokolik_village},\n",
    "        {'Inconsistent': no_match_found_between_pop_and_sokolik_ruian, 'Consistent': match_found_between_pop_and_sokolik_ruian},\n",
    "        {'RUIAN is None in Pop dataset': ruian_none_dominik_pop, 'different ruian': ruian_set_dominik_pop},\n",
    "        {'Only in Pop dataset': no_match_found_scans_for_pop, 'In both datasets': match_found_scans_for_pop},\n",
    "        {'Only in Sokolik dataset': no_match_found_scans_for_sokolik, 'In both datasets': match_found_scans_for_sokolik},\n",
    "    ]\n",
    "\n",
    "    data = [entry for entry in data if not all(value == 0 for value in entry.values())]\n",
    "\n",
    "    #print(data)\n",
    "\n",
    "    # Create a figure with four subplots\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "    # Colors for pie chart sections\n",
    "    colors = ['red', 'green']\n",
    "\n",
    "    labels = [\"Record consistency between Pop and Sokolik\", \"Village consistency\", \"Ruian consistency\", \"Reason of ruian inconsistency\", \"Record consistency between Pop and Scans\", \"Record consistency between Sokolik and Scans\"]\n",
    "\n",
    "    def autopct_func(pct):\n",
    "        total = sum(sizes)\n",
    "        absolute = int(pct / 100. * total)\n",
    "        return f'{absolute} ({pct:.1f}%)'\n",
    "    \n",
    "    # Iterate through subplots and data\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i >= len(data):\n",
    "            break\n",
    "        sizes = data[i].values()\n",
    "        ax.pie(sizes, labels=data[i].keys(), colors=colors, autopct=autopct_func, startangle=180)\n",
    "        ax.axis('equal')  # Set aspect ratio to make it a circle\n",
    "        ax.set_title(labels[i])\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    fig.suptitle(f\"Consistency check for {str(dataFrameName).split('.')[1]}\")\n",
    "    \n",
    "    # Show only the desired number of subplots (5 in this case)\n",
    "    for i in range(len(data), 6):\n",
    "        fig.delaxes(axs.flat[i])\n",
    "    # Show the combined figure with subplots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'result/{dataFrameName.value}/consistencyCheck.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return (no_match_found_between_pop_and_sokolik, ruian_none_dominik_pop, no_match_found_scans_for_pop)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConsistencyCheckOfDatasets(dataFrameName):\n",
    "\n",
    "    sig_count = 0\n",
    "\n",
    "    pop = dataframes[Dataframes.POP][dataFrameName]\n",
    "    df_pop = pop[Collections.DATASET]\n",
    "\n",
    "    pop_inv = \"invCislo\"\n",
    "    pop_sig = \"signatura\"\n",
    "\n",
    "    df_pop_key = pop_inv\n",
    "    if pop[Collections.SIZE] * 0.05 < pop[Collections.INV_NULL]:\n",
    "        df_pop_key = pop_sig\n",
    "        sig_count += 1\n",
    "    \n",
    "\n",
    "    sok_inv = \"inv_id\"\n",
    "    sok_sig = \"signature\"\n",
    "    sok = dataframes[Dataframes.SOKOLIK][dataFrameName]\n",
    "    df_sok = sok[Collections.DATASET]\n",
    "    df_sok = df_sok[df_sok[\"type\"] == \"MAT\"]\n",
    "    df_sok_key = sok_inv\n",
    "    if sok[Collections.SIZE] * 0.05 < sok[Collections.INV_NULL]:\n",
    "        df_sok_key = sok_sig\n",
    "        sig_count += 1\n",
    "\n",
    "    \n",
    "    val_inv = \"inventory_number\"\n",
    "    val_sig = \"signature\"\n",
    "    val = dataframes[Dataframes.VALUSEK][dataFrameName]\n",
    "    df_val = val[Collections.DATASET]\n",
    "    df_val_key = val_inv\n",
    "    if val[Collections.SIZE] * 0.05 < val[Collections.INV_NULL]:\n",
    "        df_val_key = val_sig\n",
    "        sig_count += 1\n",
    "\n",
    "    sca_inv = \"inv. cislo\"\n",
    "    sca_sig = \"signatura\"\n",
    "    sca = dataframes[Dataframes.SCANS][dataFrameName]\n",
    "    df_sca = sca[Collections.DATASET]\n",
    "    df_sca_key = sca_inv\n",
    "    if sca[Collections.SIZE] * 0.05 < sca[Collections.INV_NULL]:\n",
    "        df_sca_key = sca_sig\n",
    "        sig_count += 1\n",
    "\n",
    "    \n",
    "    if dataFrameName == Archives.SOA_PRAHA:\n",
    "        df_sca['signatura'] = df_sca['signatura'].str.replace('_', ' ')\n",
    "\n",
    "    if dataFrameName != Archives.MZA and dataFrameName != Archives.SOA_TREBON:\n",
    "        if(sig_count >= 2):\n",
    "            df_pop_key = pop_sig\n",
    "            df_sok_key = sok_sig\n",
    "            df_val_key = val_sig\n",
    "            df_sca_key = sca_sig\n",
    "        else:\n",
    "            df_pop_key = pop_inv\n",
    "            df_sok_key = sok_inv\n",
    "            df_val_key = val_inv\n",
    "            df_sca_key = sca_inv\n",
    "    elif dataFrameName == Archives.SOA_TREBON:\n",
    "            df_pop_key = pop_inv\n",
    "            df_sok_key = sok_sig\n",
    "            df_val_key = val_sig\n",
    "            df_sca_key = sca_inv\n",
    "\n",
    "\n",
    "    # Extract keys from each dataset\n",
    "    keys_pop = set(df_pop[df_pop_key].dropna().astype(str))\n",
    "    keys_sok = set(df_sok[df_sok_key].dropna().astype(str))\n",
    "    keys_val = set(df_val[df_val_key].dropna().astype(str).str.rstrip('.0'))\n",
    "\n",
    "    keys_sca = set()\n",
    "    if df_sca_key in df_sca:\n",
    "        keys_sca = set(df_sca[df_sca_key].dropna())\n",
    "\n",
    "    #print(type(keys_val.pop()))\n",
    "\n",
    "    sets = {\n",
    "        'POP': keys_pop,\n",
    "        'SOKOLIK': keys_sok,\n",
    "        'VALUSEK': keys_val,\n",
    "        'SCANS': keys_sca\n",
    "    }\n",
    "\n",
    "    print(\"keys pop: \", df_pop_key, \", sok: \", df_sok_key, \", val: \", df_val_key, \", sca: \", df_sca_key)\n",
    "\n",
    "    # Create the Venn diagram using venny4py\n",
    "    outPath = f'result/{dataFrameName.value}'\n",
    "    venny4py(sets=sets,size=6, dpi=600, out=outPath, ext=\"pdf\")\n",
    "    IFrame(src=outPath+\"/Venn_4.pdf\", width=600, height=400)\n",
    "\n",
    "\n",
    "    math_between_village_names = 0\n",
    "    no_math_between_village_names = 0\n",
    "\n",
    "    \n",
    "    math_between_village_ruians = 0\n",
    "    no_math_between_village_ruians = 0\n",
    "\n",
    "\n",
    "    pop_ruian_is_none = 0\n",
    "    pop_ruian_is_not_none = 0\n",
    "\n",
    "    both_ruians_are_none = 0\n",
    "    \n",
    "    df_pop['uzemi'] = df_pop['uzemi'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    for pop_item in tqdm(df_pop.itertuples(), desc=\"Processing\", unit=\"iteration\", total=pop[Collections.SIZE]):\n",
    "        for uzemi in pop_item.uzemi:\n",
    "            pop_name = uzemi[\"name\"]\n",
    "            pop_ruian = uzemi[\"ruian\"]\n",
    "\n",
    "            if pop_ruian is not None:\n",
    "                pop_ruian_is_not_none += 1\n",
    "                pop_ruian = str(pop_ruian)\n",
    "            else:\n",
    "                pop_ruian_is_none += 1\n",
    "                            \n",
    "            sok_items = df_sok[df_sok[df_sok_key] == getattr(pop_item, df_pop_key)]\n",
    "            \n",
    "            matchName = any(\n",
    "                (sok_item.obec == pop_name or sok_item.cast_obce == pop_name)\n",
    "                for sok_item in sok_items.itertuples()\n",
    "            )\n",
    "\n",
    "            if matchName:\n",
    "                math_between_village_names += 1\n",
    "            else:\n",
    "                no_math_between_village_names += 1\n",
    "\n",
    "            matchRuian = any(\n",
    "                (sok_item.obec == pop_name or sok_item.cast_obce == pop_name) and\n",
    "                ((sok_item.obec_ruian == pop_ruian and pop_ruian) or \n",
    "                 (sok_item.cast_obce_ruian == pop_ruian and pop_ruian))\n",
    "                for sok_item in sok_items.itertuples()\n",
    "            )\n",
    "\n",
    "            if matchRuian:\n",
    "                math_between_village_ruians += 1\n",
    "            else:\n",
    "                no_math_between_village_ruians += 1\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"math_between_village_names: \", math_between_village_names, \" vs \", \"no_math_between_village_names: \", no_math_between_village_names)\n",
    "    print(\"math_between_village_ruians: \", math_between_village_ruians, \" vs \", \"no_math_between_village_ruians: \", no_math_between_village_ruians)\n",
    "    print(\"pop_ruian_is_none: \", pop_ruian_is_none, \" vs \", \"pop_ruian_is_not_none: \", pop_ruian_is_not_none)\n",
    "    print(\"both_ruians_are_none: \", both_ruians_are_none)\n",
    "\n",
    "    categories = ['Between Village Names', 'Between Village Ruians', 'Population Ruian']\n",
    "    math_counts = [math_between_village_names, math_between_village_ruians, pop_ruian_is_not_none]\n",
    "    no_math_counts = [no_math_between_village_names, no_math_between_village_ruians, pop_ruian_is_none]\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(categories))\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar1 = plt.bar(index, math_counts, bar_width, label='Match', color='green')\n",
    "    bar2 = plt.bar(index + bar_width, no_math_counts, bar_width, label='No Match', color='red')\n",
    "\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Results of Consistency Check')\n",
    "    plt.xticks(index + bar_width / 2, categories)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'result/{dataFrameName.value}/consistencyCheck.pdf', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    categories = ['Village name match failed', 'Ruian match failed', 'Match name and ruian']\n",
    "    values = [no_math_between_village_names, no_math_between_village_ruians - no_math_between_village_names, math_between_village_ruians]  # Replace these values with your data\n",
    "\n",
    "    # Plotting the pie chart\n",
    "    plt.pie(values, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "    # Aspect ratio to ensure that the pie is drawn as a circle\n",
    "    plt.axis('equal')\n",
    "\n",
    "    # Title of the pie chart\n",
    "    plt.title('Consistency check')\n",
    "\n",
    "    plt.savefig(f'result/{dataFrameName.value}/consistencyCheck2.pdf', format='pdf')\n",
    "    # Display the chart\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moravský zemský archiv\n",
    "There is a mapping between the datasets; the actapublica website uses the book number designation. One author interpreted this as an inventory number and the other as a signature. The second figure has both datasets null for all records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ObjectComparisonPrint(Archives.MZA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values analyzation\n",
    "The analysis shows that Dominik Pop has an inventory number for each record and Jakub Sokolík has a signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AnalyzeMissingValues(Archives.MZA)\n",
    "#AnalyzeDuplicates(Archives.MZA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature to inventory number matching\n",
    "We want to see if one dataset is a subset of another. For the records we find in both datasets, we want to check how much difference there is in the ruian numbers of the municipalities found. We also want to see for how many records we can find scans from the third dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConsistencyCheckOfDatasets(Archives.MZA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for archive in Archives:\n",
    "    display(Markdown(f\"## {str(archive).split('.')[1]}\"))\n",
    "    #ObjectComparisonPrint(archive)\n",
    "    AnalyzeMissingValues(archive)\n",
    "    AnalyzeDuplicates(archive)\n",
    "    ConsistencyCheckOfDatasets(archive)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Datasets\n",
    "#### Dominik Pop\n",
    "dynamic key name of the municipality, had to convert for analysis.\n",
    "#### Jakub Sokolik\n",
    "It has information about RUIAN that is missing in Dominik Popa's dataset.\n",
    "#### Data from flash disk\n",
    "Hradec and Plzen missing\n",
    "Litomerice,Opava, SOA_PRAHA corrupted file\n",
    "Trebon missing scans\n",
    "Actapublica was the only one that had output in XML format, so I wrote a script to convert the file to JSON format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
